{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKpkqh52o8o9"
      },
      "source": [
        "# **Connect to drive, import libraries, run functions, and define first variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4YxboNQ5u0f",
        "outputId": "a64ea438-f5d5-4719-d7fd-20fd291eaab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzxZrKzPs0im"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import datetime\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import seaborn as sns\n",
        "import os.path\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.lines as mlines\n",
        "import calendar\n",
        "import joblib\n",
        "import pickle\n",
        "import random\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "from scipy.stats import ttest_ind\n",
        "import time\n",
        "\n",
        " # Import the ThreadPoolExecutor\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import Dropout\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkkqgGvQFYn9"
      },
      "source": [
        "# **ML comparison using BRF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1iu6tmAFDLz"
      },
      "source": [
        "## **preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og_WXnMWHJ3i"
      },
      "outputs": [],
      "source": [
        "Total_subset = np.load('/content/drive/MyDrive/PHD/GOES and GPP/Data ready for ML/Data_Aug22_BRFandCMI.npy', allow_pickle=True).item()\n",
        "Aim = 'GPP_np' # 'GPP_np' and 'RECO_np'\n",
        "MinSiteSample = 300\n",
        "shuffle_irt = 10\n",
        "SAVINGaddress = '/content/drive/MyDrive/Ameriflux_GOES/Gap filling manuscript/Results and Figures/accuracies/'+Aim+'/'\n",
        "SMF_BRFs = ['BRF1', 'BRF2', 'BRF3', 'BRF5', 'BRF6', 'NIRv', 'NIRvP', 'DSR']\n",
        "GMF_5BRFs = ['BRF1', 'BRF2', 'BRF3', 'BRF5', 'BRF6', 'NIRv', 'NIRvP', 'DSR']\n",
        "# General Modeling Features (GMF)\n",
        "GMF_CMIs = ['CMI_C01','CMI_C02', 'CMI_C03', 'CMI_C04', 'CMI_C05', 'CMI_C06', 'CMI_C07', 'CMI_C08', 'CMI_C09', 'CMI_C10',\n",
        "                 'CMI_C11', 'CMI_C12', 'CMI_C13', 'CMI_C14', 'CMI_C15', 'CMI_C16']\n",
        "\n",
        "New_Total_subset = []\n",
        "tt = 1\n",
        "maxSamples = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja1YXbbvGHdd"
      },
      "source": [
        "## **RF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czqqrq-jFY4r"
      },
      "outputs": [],
      "source": [
        "# Lists to store the results\n",
        "R2test_RF = []\n",
        "RMSEtest_RF = []\n",
        "MeanCVR2_RF = []\n",
        "# Define hyperparameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 250, 500, 800],\n",
        "    'max_depth': [5, 10, 14, 20, 24],\n",
        "}\n",
        "\n",
        "num = 1\n",
        "for itr in range(shuffle_irt):\n",
        "  for i in range(len(Total_subset)):\n",
        "      SiteData = Total_subset[i]\n",
        "      print(SiteData.shape)\n",
        "\n",
        "      if MinSiteSample < SiteData.shape[0]:\n",
        "          # Split into training and testing sets using one-holdout (20% for testing)\n",
        "          train_data, test_data = train_test_split(SiteData, test_size=0.2, shuffle=True)\n",
        "\n",
        "          # Separate target variable and predictors\n",
        "          X_train = train_data[SMF_BRFs]\n",
        "          y_train = train_data[Aim]\n",
        "          X_test = test_data[SMF_BRFs]\n",
        "          y_test = test_data[Aim]\n",
        "\n",
        "          # Train Random Forest model with grid search and cross-validation\n",
        "          rf_model = RandomForestRegressor()\n",
        "          grid_search = GridSearchCV(rf_model, param_grid, cv=4, scoring='r2')\n",
        "          grid_search.fit(X_train, y_train)\n",
        "\n",
        "          # Get the best model from grid search and make predictions on test set\n",
        "          best_rf_model = grid_search.best_estimator_\n",
        "          y_test_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "          best_params = grid_search.best_params_\n",
        "          best_r2_cv = grid_search.best_score_\n",
        "          print(\"Best Parameters:\", best_params)\n",
        "          print(\"Mean R2 from CV:\", best_r2_cv)\n",
        "          MeanCVR2_RF.append(best_r2_cv)\n",
        "\n",
        "          # Compute R2 and RMSE for testing set\n",
        "          test_r2 = r2_score(y_test, y_test_pred)\n",
        "          test_RMSE = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "          num += 1\n",
        "          print(num, ' : R2_te =', test_r2, 'RMSE_te =', test_RMSE)\n",
        "\n",
        "          # Append accuracies to lists\n",
        "          R2test_RF.append(test_r2)\n",
        "          RMSEtest_RF.append(test_RMSE)\n",
        "\n",
        "\n",
        "rf_accuracy = {'R2rf': R2test_RF,\n",
        "            'RMSErf': RMSEtest_RF,\n",
        "            'CVR2rf': MeanCVR2_RF}\n",
        "\n",
        "# Saving the numpy array to a file\n",
        "file_path_accuracy = SAVINGaddress+'rf_accuracy.npy'\n",
        "np.save(file_path_accuracy, rf_accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.boxplot(R2test_RF)\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm1hOyq0GS1w"
      },
      "source": [
        "## **SVR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFvfVdmdA9b2"
      },
      "outputs": [],
      "source": [
        "# Lists to store the results\n",
        "R2test_SVR = []\n",
        "RMSEtest_SVR = []\n",
        "MeanCVR2_SVR = []\n",
        "# Define hyperparameter grid for SVR\n",
        "param_grid = {\n",
        "    'C': [0.1, 10, 100, 1000],\n",
        "    'epsilon': [0.001, 0.01, 0.2],\n",
        "    'kernel': ['rbf', 'sigmoid', 'poly', 'linear']\n",
        "}\n",
        "\n",
        "num = 1\n",
        "for itr in range(shuffle_irt):\n",
        "  for i in range(len(Total_subset)):\n",
        "      SiteData = Total_subset[i]\n",
        "      print(SiteData.shape)\n",
        "\n",
        "      if MinSiteSample < SiteData.shape[0]:\n",
        "          # Split into training and testing sets using one-holdout (20% for testing)\n",
        "          train_data, test_data = train_test_split(SiteData, test_size=0.2, shuffle=True)\n",
        "\n",
        "          # Separate target variable and predictors\n",
        "          X_train = train_data[SMF_BRFs]\n",
        "          y_train = train_data[Aim]\n",
        "          X_test = test_data[SMF_BRFs]\n",
        "          y_test = test_data[Aim]\n",
        "\n",
        "          # Train SVR model with grid search and cross-validation\n",
        "          svr_model = SVR(kernel = \"rbf\")\n",
        "          grid_search = GridSearchCV(svr_model, param_grid, cv=4, scoring='r2')\n",
        "          grid_search.fit(X_train, y_train)\n",
        "\n",
        "          # Get the best model from grid search and its best parameters\n",
        "          best_svr_model = grid_search.best_estimator_\n",
        "          best_params = grid_search.best_params_\n",
        "          best_r2_cv = grid_search.best_score_\n",
        "          print(\"Best Parameters:\", best_params)\n",
        "          print(\"Mean R2 from CV:\", best_r2_cv)\n",
        "          MeanCVR2_SVR.append(best_r2_cv)\n",
        "\n",
        "          # Make predictions on test set using the best model\n",
        "          y_test_pred = best_svr_model.predict(X_test)\n",
        "\n",
        "          # Compute R2 and RMSE for testing set\n",
        "          test_r2 = r2_score(y_test, y_test_pred)\n",
        "          test_RMSE = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "          num += 1\n",
        "          print(num, ' : R2_te =', test_r2, 'RMSE_te =', test_RMSE)\n",
        "\n",
        "          # Append accuracies to lists\n",
        "          R2test_SVR.append(test_r2)\n",
        "          RMSEtest_SVR.append(test_RMSE)\n",
        "\n",
        "SVR_accuracy = {'R2SVR': R2test_SVR,\n",
        "                'RMSESVR': RMSEtest_SVR,\n",
        "                'CVSVR': MeanCVR2_SVR}\n",
        "\n",
        "# Saving the numpy array to a file\n",
        "file_path_accuracy = SAVINGaddress+'SVR_accuracy.npy'\n",
        "np.save(file_path_accuracy, SVR_accuracy)\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "plt.boxplot(R2test_SVR)\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-sCftr4GT3c"
      },
      "source": [
        "## **MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTmrFcLnCAkc"
      },
      "outputs": [],
      "source": [
        "# Lists to store the results\n",
        "R2test_MLP = []\n",
        "RMSEtest_MLP = []\n",
        "MeanCVR2_MLP = []\n",
        "# Define hyperparameter grid for MLP\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (50, 50), (50, 50, 50), (100,), (100, 100), (100, 100, 100), (200,), (200, 200), (200, 200, 200), (300,),\n",
        " (300, 300), (300, 300, 300), (500,), (500, 500), (500, 500, 500)],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'activation': ['logistic', 'tanh', 'relu'],\n",
        "    'solver': ['adam','sgd'],\n",
        "    'Alpha': [0.0001, 0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "num = 1\n",
        "for itr in range(shuffle_irt):\n",
        "  for i in range(len(Total_subset)):\n",
        "      SiteData = Total_subset[i]\n",
        "      print(SiteData.shape)\n",
        "\n",
        "      if MinSiteSample < SiteData.shape[0]:\n",
        "          # Split into training and testing sets using one-holdout (20% for testing)\n",
        "          train_data, test_data = train_test_split(SiteData, test_size=0.2, shuffle=True)\n",
        "\n",
        "          # Separate target variable and predictors\n",
        "          X_train = train_data[SMF_BRFs]\n",
        "          y_train = train_data[Aim]\n",
        "          X_test = test_data[SMF_BRFs]\n",
        "          y_test = test_data[Aim]\n",
        "\n",
        "          # Train MLP model with grid search and cross-validation\n",
        "          mlp_model = MLPRegressor(max_iter=1000)\n",
        "          grid_search = GridSearchCV(mlp_model, param_grid, cv=4, scoring='r2')\n",
        "          grid_search.fit(X_train, y_train)\n",
        "\n",
        "          # Get the best model from grid search and its best parameters\n",
        "          best_mlp_model = grid_search.best_estimator_\n",
        "          best_params = grid_search.best_params_\n",
        "          best_r2_cv = grid_search.best_score_\n",
        "          print(\"Best Parameters:\", best_params)\n",
        "          print(\"Mean R2 from CV:\", best_r2_cv)\n",
        "          MeanCVR2_MLP.append(best_r2_cv)\n",
        "          # Make predictions on test set using the best model\n",
        "          y_test_pred = best_mlp_model.predict(X_test)\n",
        "\n",
        "          # Compute R2 and RMSE for testing set\n",
        "          test_r2 = r2_score(y_test, y_test_pred)\n",
        "          test_RMSE = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "          num += 1\n",
        "          print(num, ' : R2_te =', test_r2, 'RMSE_te =', test_RMSE)\n",
        "\n",
        "          # Append accuracies to lists\n",
        "          R2test_MLP.append(test_r2)\n",
        "          RMSEtest_MLP.append(test_RMSE)\n",
        "\n",
        "MLP_accuracy = {'R2MLP': R2test_MLP,\n",
        "                'RMSEMLP': RMSEtest_MLP,\n",
        "                'CVMLP': MeanCVR2_MLP}\n",
        "\n",
        "# Saving the numpy array to a file\n",
        "file_path_accuracy = SAVINGaddress+'MLP_accuracy.npy'\n",
        "np.save(file_path_accuracy, MLP_accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.boxplot(R2test_MLP)\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5015GTXGVK5"
      },
      "source": [
        "## **GBR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFA_HipDCkIn"
      },
      "outputs": [],
      "source": [
        "# Lists to store the results\n",
        "R2test_GBR = []\n",
        "RMSEtest_GBR = []\n",
        "MeanCVR2_GBR = []\n",
        "FI_all = []\n",
        "# Define hyperparameter grid for GBR\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 400],\n",
        "    'max_depth': [3, 5, 8, 10],\n",
        "    'learning_rate': [0.01,0.05,0.1]\n",
        "}\n",
        "\n",
        "FI_BRFs = []\n",
        "num = 1\n",
        "for itr in range(shuffle_irt):\n",
        "  for i in range(len(Total_subset)):\n",
        "      SiteData = Total_subset[i]\n",
        "      print(SiteData.shape)\n",
        "\n",
        "      if MinSiteSample < SiteData.shape[0]:\n",
        "          # Split into training and testing sets using one-holdout (20% for testing)\n",
        "          train_data, test_data = train_test_split(SiteData, test_size=0.2, shuffle=True)\n",
        "\n",
        "          # Separate target variable and predictors\n",
        "          X_train = train_data[SMF_BRFs]\n",
        "          y_train = train_data[Aim]\n",
        "          X_test = test_data[SMF_BRFs]\n",
        "          y_test = test_data[Aim]\n",
        "\n",
        "          # Train GBR model with grid search and cross-validation\n",
        "          gbr_model = GradientBoostingRegressor()\n",
        "          grid_search = GridSearchCV(gbr_model, param_grid, cv=4, scoring='r2')\n",
        "          grid_search.fit(X_train, y_train)\n",
        "\n",
        "          # Get the best model from grid search and its best parameters\n",
        "          best_gbr_model = grid_search.best_estimator_\n",
        "          best_params = grid_search.best_params_\n",
        "          best_r2_cv = grid_search.best_score_\n",
        "          print(\"Best Parameters:\", best_params)\n",
        "          print(\"Mean R2 from CV:\", best_r2_cv)\n",
        "          MeanCVR2_GBR.append(best_r2_cv)\n",
        "\n",
        "          # Make predictions on test set using the best model\n",
        "          y_test_pred = best_gbr_model.predict(X_test)\n",
        "\n",
        "          # Compute R2 and RMSE for testing set\n",
        "          test_r2 = r2_score(y_test, y_test_pred)\n",
        "          test_RMSE = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "          num += 1\n",
        "          print(num, ' : R2_te =', test_r2, 'RMSE_te =', test_RMSE)\n",
        "\n",
        "          R2test_GBR.append(test_r2)\n",
        "          RMSEtest_GBR.append(test_RMSE)\n",
        "              # store feature importance\n",
        "          FI_BRFs = pd.DataFrame()\n",
        "          FI_BRFs['feature'] = SMF_BRFs\n",
        "\n",
        "          FI_BRFs['score'] = best_gbr_model.feature_importances_\n",
        "          FI_all.append(FI_BRFs)\n",
        "\n",
        "          # Sort the groups by mean score in descending order\n",
        "          sorted_df = FI_BRFs.sort_values('score', ascending=False)\n",
        "          # Plotting the feature importance\n",
        "          plt.figure(figsize=(2, 5))\n",
        "          plt.barh(sorted_df['feature'], sorted_df['score'], color='black')\n",
        "          plt.xlabel('Mean Score')\n",
        "          plt.ylabel('Feature')\n",
        "          plt.title('Feature Importance')\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "plt.boxplot(R2test_GBR)\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n",
        "\n",
        "# Concatenate all dataframes in the list into one dataframe\n",
        "BRFs_df = pd.concat(FI_all)\n",
        "# Group by 'feature' and calculate the mean score\n",
        "grouped_df = BRFs_df.groupby('feature')['score'].mean().reset_index()\n",
        "# Sort the groups by mean score in descending order\n",
        "sorted_df = grouped_df.sort_values('score', ascending=False)\n",
        "# Plotting the feature importance\n",
        "plt.figure(figsize=(2, 1.3))\n",
        "plt.barh(sorted_df['feature'], sorted_df['score'], color='black')\n",
        "plt.xlabel('Mean Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwF5Dwo2N_2A"
      },
      "source": [
        "# **CMI and BRF comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smSShyflMsVQ"
      },
      "outputs": [],
      "source": [
        "Total_subset = np.load('/content/drive/MyDrive/PHD/GOES and GPP/Data ready for ML/Data_Aug17_BRFandCMI.npy', allow_pickle=True).item()\n",
        "\n",
        "Aims = ['RECO_np', 'GPP_np'] # 'GPP_np' and 'RECO_np'\n",
        "MinSiteSample = 100\n",
        "CMI_18b = ['CMI_C01','CMI_C02', 'CMI_C03', 'CMI_C04', 'CMI_C05', 'CMI_C06', 'CMI_C07', 'CMI_C08', 'CMI_C09', 'CMI_C10',\n",
        "          'CMI_C11', 'CMI_C12', 'CMI_C13', 'CMI_C14', 'CMI_C15', 'CMI_C16']\n",
        "BRF_6b = ['BRF1', 'BRF2', 'BRF3', 'BRF5', 'BRF6']\n",
        "Com_f = CMI_18b + BRF_6b\n",
        "\n",
        "data = pd.concat(Total_subset)\n",
        "\n",
        "print(len(Total_subset))\n",
        "print(len(Total_DataCMI))\n",
        "\n",
        "def train_and_test_model(data, features, target):\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=True)\n",
        "\n",
        "    X_train = train_data[features]\n",
        "    y_train = train_data[target]\n",
        "    X_test = test_data[features]\n",
        "    y_test = test_data[target]\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200, 400],\n",
        "        'max_depth': [3, 5, 8, 10],\n",
        "        'learning_rate': [0.01,0.05,0.1]\n",
        "    }\n",
        "\n",
        "    # Train GBR model with grid search and cross-validation\n",
        "    gbr_model = GradientBoostingRegressor()\n",
        "    grid_search = GridSearchCV(gbr_model, param_grid, cv=4, scoring='r2')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    model = grid_search.best_estimator_\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    train_RMSE = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    test_RMSE = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "    print(train_r2, train_RMSE, test_r2, test_RMSE)\n",
        "\n",
        "    return train_r2, train_RMSE, test_r2, test_RMSE, model.feature_importances_\n",
        "\n",
        "R2_BRF_RECO, RMSE_BRF_RECO, R2_CMI_RECO, RMSE_CMI_RECO, R2_Com_RECO, RMSE_Com_RECO  = [], [], [], [], [], []\n",
        "R2_BRF_GPP, RMSE_BRF_GPP, R2_CMI_GPP, RMSE_CMI_GPP, R2_Com_GPP, RMSE_Com_GPP = [], [], [], [], [], []\n",
        "all_FI_BRFs, all_FI_CMIs, all_FI_Coms = [], [], []\n",
        "\n",
        "for Aim in Aims:\n",
        "    R2_BRF, RMSE_BRF, R2_CMI, RMSE_CMI , R2_Com, RMSE_Com = [], [], [], [], [], []\n",
        "\n",
        "    for Isite in range(101):\n",
        "        print(SiteData)\n",
        "        selected_data = Total_subset[Isite]\n",
        "\n",
        "        if MinSiteSample < selected_data.shape[0]:\n",
        "\n",
        "            r2_BRF_tr, rmse_BRF_tr, r2_BRF_te, rmse_BRF_te, FI_BRF = train_and_test_model(selected_data, BRF_6b, Aim)\n",
        "            r2_CMI_tr, rmse_CMI_tr, r2_CMI_te, rmse_CMI_te, FI_CMI = train_and_test_model(selected_data, CMI_18b, Aim)\n",
        "            r2_Com_tr, rmse_Com_tr, r2_Com_te, rmse_Com_te, FI_Com = train_and_test_model(selected_data, Com_f, Aim)\n",
        "\n",
        "            R2_BRF.append(r2_BRF_te)\n",
        "            RMSE_BRF.append(rmse_BRF_te)\n",
        "\n",
        "            R2_CMI.append(r2_CMI_te)\n",
        "            RMSE_CMI.append(rmse_CMI_te)\n",
        "\n",
        "            R2_Com.append(r2_Com_te)\n",
        "            RMSE_Com.append(rmse_Com_te)\n",
        "\n",
        "        # store feature importance\n",
        "        FI_BRFs = pd.DataFrame()\n",
        "        FI_BRFs['feature'] = BRF_6b\n",
        "        FI_BRFs['score'] = FI_BRF\n",
        "        all_FI_BRFs.append(FI_BRFs)\n",
        "\n",
        "        FI_CMIs = pd.DataFrame()\n",
        "        FI_CMIs['feature'] = CMI_18b\n",
        "        FI_CMIs['score'] = FI_CMI\n",
        "        all_FI_CMIs.append(FI_CMIs)\n",
        "\n",
        "        FI_Coms = pd.DataFrame()\n",
        "        FI_Coms['feature'] = Com_f\n",
        "        FI_Coms['score'] = FI_Com\n",
        "        all_FI_Coms.append(FI_Coms)\n",
        "\n",
        "    if Aim == 'RECO_np':\n",
        "        R2_BRF_RECO = R2_BRF\n",
        "        RMSE_BRF_RECO = RMSE_BRF\n",
        "        R2_CMI_RECO = R2_CMI\n",
        "        RMSE_CMI_RECO = RMSE_CMI\n",
        "        R2_Com_RECO = R2_Com\n",
        "        RMSE_Com_RECO = RMSE_Com\n",
        "\n",
        "    if Aim == 'GPP_np':\n",
        "        R2_BRF_GPP = R2_BRF\n",
        "        RMSE_BRF_GPP = RMSE_BRF\n",
        "        R2_CMI_GPP = R2_CMI\n",
        "        RMSE_CMI_GPP = RMSE_CMI\n",
        "        R2_Com_GPP = R2_Com\n",
        "        RMSE_Com_GPP = RMSE_Com\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvoyhazPOOuf"
      },
      "outputs": [],
      "source": [
        "SAVINGaddress = '/content/drive/MyDrive/Ameriflux_GOES/Gap filling manuscript/Results and Figures/accuracies/BRF_CMI/'\n",
        "\n",
        "accuracy = {\"R2_BRF_RECO\": R2_BRF_RECO,\n",
        "    \"RMSE_BRF_RECO\": RMSE_BRF_RECO,\n",
        "    \"R2_CMI_RECO\": R2_CMI_RECO,\n",
        "    \"RMSE_CMI_RECO\": RMSE_CMI_RECO,\n",
        "    \"R2_Com_RECO\": R2_Com_RECO,\n",
        "    \"RMSE_Com_RECO\": RMSE_Com_RECO,\n",
        "    \"R2_BRF_GPP\": R2_BRF_GPP,\n",
        "    \"RMSE_BRF_GPP\": RMSE_BRF_GPP,\n",
        "    \"R2_CMI_GPP\": R2_CMI_GPP,\n",
        "    \"RMSE_CMI_GPP\": RMSE_CMI_GPP,\n",
        "    \"R2_Com_GPP\": R2_Com_GPP,\n",
        "    \"RMSE_Com_GPP\": RMSE_Com_GPP}\n",
        "\n",
        "# Saving the numpy array to a file\n",
        "file_path_accuracy = SAVINGaddress+'accuracyBRF_CMI.npy'\n",
        "np.save(file_path_accuracy, accuracy)\n",
        "\n",
        "\n",
        "# Calculate mean and standard deviation for each list\n",
        "listsR2 = [R2_BRF_RECO, R2_CMI_RECO, R2_Com_RECO, R2_BRF_GPP, R2_CMI_GPP, R2_Com_GPP]\n",
        "listsRMSE = [RMSE_BRF_RECO, RMSE_CMI_RECO, RMSE_Com_RECO, RMSE_BRF_GPP, RMSE_CMI_GPP, RMSE_Com_GPP]\n",
        "\n",
        "# Replace the original values in the rf_accuracy dictionary with the filtered lists\n",
        "accuracy1 = {\n",
        "    \"R2_BRF_RECO\": listsR2[0],\n",
        "    \"RMSE_BRF_RECO\": listsRMSE[0],\n",
        "    \"R2_CMI_RECO\": listsR2[1],\n",
        "    \"RMSE_CMI_RECO\": listsRMSE[1],\n",
        "    \"R2_Com_RECO\": listsR2[2],\n",
        "    \"RMSE_Com_RECO\": listsRMSE[2],\n",
        "    \"R2_BRF_GPP\": listsR2[3],\n",
        "    \"RMSE_BRF_GPP\": listsRMSE[3],\n",
        "    \"R2_CMI_GPP\": listsR2[4],\n",
        "    \"RMSE_CMI_GPP\": listsRMSE[4],\n",
        "    \"R2_Com_GPP\": listsR2[5],\n",
        "    \"RMSE_Com_GPP\": listsRMSE[5]\n",
        "}\n",
        "\n",
        "# Saving the numpy array to a file\n",
        "file_path_accuracy = SAVINGaddress+'accuracyBRF_CMI_V1.npy'\n",
        "np.save(file_path_accuracy, accuracy1)\n",
        "\n",
        "FI_save = { \"all_FI_BRFs\": all_FI_BRFs,\n",
        "    \"all_FI_CMIs\": all_FI_CMIs,\n",
        "    \"all_FI_Coms\": all_FI_Coms}\n",
        "\n",
        "# Saving the numpy array to a file\n",
        "file_path_accuracy = SAVINGaddress+'FI_save.npy'\n",
        "np.save(file_path_accuracy, FI_save)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLBzCL4NhPn3"
      },
      "outputs": [],
      "source": [
        "def create_box_plots(ax, data1, data2, data3, labels, ylabel, legend_labels, box_colors, box_hatch):\n",
        "    bp1 = ax.boxplot(data1, positions=[1, 4], labels=labels, widths=0.3, patch_artist=True)\n",
        "    bp2 = ax.boxplot(data2, positions=[1.7, 4.7], labels=labels, widths=0.3, patch_artist=True)\n",
        "    bp3 = ax.boxplot(data3, positions=[2.4, 5.4], labels=labels, widths=0.3, patch_artist=True)\n",
        "\n",
        "    for bp in [bp1, bp2, bp3]:\n",
        "        for box in bp['boxes']:\n",
        "            box.set(color='black', linewidth=1)\n",
        "            if bp == bp1:\n",
        "                box.set(facecolor=box_colors)\n",
        "            elif bp == bp2:\n",
        "                box.set(facecolor='white')\n",
        "            else :\n",
        "                box.set(hatch=box_hatch)\n",
        "                box.set(facecolor='white')\n",
        "\n",
        "    legend_patches = [mpatches.Patch(color=box_colors, label=legend_labels[0]),\n",
        "                      mpatches.Patch(facecolor='white', edgecolor='black', label=legend_labels[1]),\n",
        "                      mpatches.Patch(hatch=box_hatch, facecolor='white', edgecolor='black', label=legend_labels[2])]\n",
        "\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.legend(handles=legend_patches)\n",
        "    ax.set_xticks([1.7, 4.7])\n",
        "    ax.set_xticklabels(labels)\n",
        "\n",
        "# Define the path to the saved Numpy file\n",
        "file_path_accuracy = '/content/drive/MyDrive/Ameriflux_GOES/Gap filling manuscript/Results and Figures/accuracies/BRF_CMI/accuracyBRF_CMI_V1.npy'\n",
        "\n",
        "# Load the dictionary from the Numpy file\n",
        "loaded_accuracy = np.load(file_path_accuracy, allow_pickle=True).item()\n",
        "\n",
        "# Extract the values from the loaded dictionary\n",
        "R2_BRF_RECO = loaded_accuracy[\"R2_BRF_RECO\"]\n",
        "RMSE_BRF_RECO = loaded_accuracy[\"RMSE_BRF_RECO\"]\n",
        "R2_CMI_RECO = loaded_accuracy[\"R2_CMI_RECO\"]\n",
        "RMSE_CMI_RECO = loaded_accuracy[\"RMSE_CMI_RECO\"]\n",
        "R2_Com_RECO = loaded_accuracy[\"R2_Com_RECO\"]\n",
        "RMSE_Com_RECO = loaded_accuracy[\"RMSE_Com_RECO\"]\n",
        "R2_BRF_GPP = loaded_accuracy[\"R2_BRF_GPP\"]\n",
        "RMSE_BRF_GPP = loaded_accuracy[\"RMSE_BRF_GPP\"]\n",
        "R2_CMI_GPP = loaded_accuracy[\"R2_CMI_GPP\"]\n",
        "RMSE_CMI_GPP = loaded_accuracy[\"RMSE_CMI_GPP\"]\n",
        "R2_Com_GPP = loaded_accuracy[\"R2_Com_GPP\"]\n",
        "RMSE_Com_GPP = loaded_accuracy[\"RMSE_Com_GPP\"]\n",
        "\n",
        "# Set up the figure and the axes\n",
        "fig, axes = plt.subplots(2, 1, figsize=(3, 6))\n",
        "\n",
        "labels = ['GPP', 'RECO']\n",
        "box_colors_GPP = 'black'\n",
        "box_hatch_RECO = '//'\n",
        "legend_labels = ['BRF', 'CMI', 'BRF+CMI']\n",
        "\n",
        "# Create the box plots for R2\n",
        "create_box_plots(axes[0], [R2_BRF_GPP, R2_BRF_RECO], [R2_CMI_GPP, R2_CMI_RECO], [R2_Com_GPP, R2_Com_RECO], labels, '$R^2$', legend_labels, box_colors_GPP, box_hatch_RECO)\n",
        "# Create the box plots for RMSE\n",
        "create_box_plots(axes[1], [RMSE_BRF_GPP, RMSE_BRF_RECO], [RMSE_CMI_GPP, RMSE_CMI_RECO], [RMSE_Com_GPP, RMSE_Com_RECO], labels, 'RMSE (Âµmol $CO_2$ $m^{-2}$ $s^{-1}$)', legend_labels, box_colors_GPP, box_hatch_RECO)\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWDqw42UOOMl"
      },
      "outputs": [],
      "source": [
        "# Concatenate all dataframes in the list into one dataframe\n",
        "BRFs_df = pd.concat(all_FI_BRFs)\n",
        "# Group by 'feature' and calculate the mean score\n",
        "grouped_df = BRFs_df.groupby('feature')['score'].mean().reset_index()\n",
        "# Sort the groups by mean score in descending order\n",
        "sorted_df = grouped_df.sort_values('score', ascending=False)\n",
        "# Plotting the feature importance\n",
        "plt.figure(figsize=(2, 1.3))\n",
        "plt.barh(sorted_df['feature'], sorted_df['score'], color='black')\n",
        "plt.xlabel('Mean Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n",
        "\n",
        "# Concatenate all dataframes in the list into one dataframe\n",
        "CMIs_df = pd.concat(all_FI_CMIs)\n",
        "# Group by 'feature' and calculate the mean score\n",
        "grouped_df = CMIs_df.groupby('feature')['score'].mean().reset_index()\n",
        "# Sort the groups by mean score in descending order\n",
        "sorted_df = grouped_df.sort_values('score', ascending=False)\n",
        "# Plotting the feature importance\n",
        "plt.figure(figsize=(2, 4))\n",
        "plt.barh(sorted_df['feature'], sorted_df['score'], color='black')\n",
        "plt.xlabel('Mean Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n",
        "\n",
        "# Concatenate all dataframes in the list into one dataframe\n",
        "Coms_df = pd.concat(all_FI_Coms)\n",
        "# Group by 'feature' and calculate the mean score\n",
        "grouped_df = Coms_df.groupby('feature')['score'].mean().reset_index()\n",
        "# Sort the groups by mean score in descending order\n",
        "sorted_df = grouped_df.sort_values('score', ascending=False)\n",
        "# Plotting the feature importance\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.barh(sorted_df['feature'], sorted_df['score'], color='black')\n",
        "plt.xlabel('Mean Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeSiJTyI-r3G"
      },
      "source": [
        "# **Different Gap size**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehgdlJRdfwfi"
      },
      "source": [
        "## four years training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt4HR55qA8L4"
      },
      "source": [
        "### preprocessing and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vryUYODw-tHz"
      },
      "outputs": [],
      "source": [
        "Total_subset = np.load('/content/drive/MyDrive/PHD/GOES and GPP/Data ready for ML/Subset_Jun21.npy', allow_pickle=True).item()\n",
        "MinSiteSample = 2000\n",
        "Sizes = []\n",
        "AllData = []\n",
        "# Create lists to store the accuracies and scores\n",
        "All_traindata = []\n",
        "All_testdata = []\n",
        "num = 1\n",
        "Aim = \"RECO_np\" # \"GPP_np\", \"RECO_np\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OQWUvsgHi8I",
        "outputId": "356716ef-c79d-49db-fbd7-3382b2fcc437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "def split_data(df, test_scenario):\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    if test_scenario == 1:\n",
        "        # Generate test_days with groups having at least two samples\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) > 2)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=4000, replace=True)\n",
        "        # Filter test_data and ensure each group has at least two samples\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days)].groupby('y_doy').apply(lambda x: x.sample(n=2)).copy()\n",
        "\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index.get_level_values(None))\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 2:\n",
        "        # Generate test_days with groups having at least two samples\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) > 3)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=2000, replace=True)\n",
        "        # Filter test_data and ensure each group has at least two samples\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days)].groupby('y_doy').apply(lambda x: x.sample(n=3)).copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index.get_level_values(None))\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 3:\n",
        "        # Generate test_days with groups having at least four samples\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) > 4)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=1200, replace=True)\n",
        "        # Filter test_data and ensure each group has at least four samples\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days)].groupby('y_doy').apply(lambda x: x.sample(n=4)).copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index.get_level_values(None))\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 4:\n",
        "        # Generate test_days with groups having at least seven samples\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) > 8)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=500, replace=True)\n",
        "        # Filter test_data and ensure each group has at least seven samples\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days)].groupby('y_doy').apply(lambda x: x.sample(n=8)).copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index.get_level_values(None))\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 5:\n",
        "        # Generate test_days with groups having at least ten samples\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 8)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=500, replace=True)\n",
        "        # Filter test_data and ensure each group has at least ten samples\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days)].groupby('y_doy').apply(lambda x: x.sample(n=8)).copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index.get_level_values(None))\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 6:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 8)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=57, replace=True)\n",
        "        # Include one day before and after the selected y_doy\n",
        "        test_days_extended = []\n",
        "        for day in test_days:\n",
        "            test_days_extended.extend([day-1, day, day+1, day+2])\n",
        "        test_days_extended = np.array(test_days_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected y_doy and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 7:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 8)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=30, replace=True)\n",
        "        # Include two days before and after the selected y_doy\n",
        "        test_days_extended = []\n",
        "        for day in test_days:\n",
        "            test_days_extended.extend([day-3, day-2, day-1, day, day+1, day+2, day+3])\n",
        "        test_days_extended = np.array(test_days_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected y_doy and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 8:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_doy')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 8)\n",
        "        test_days = np.random.choice(valid_groups['y_doy'].unique(), size=17, replace=True)\n",
        "        # Include three days before and after the selected y_doy\n",
        "        test_days_extended = []\n",
        "        for day in test_days:\n",
        "            test_days_extended.extend([day-7, day-6, day-5, day-4, day-3, day-2, day-1, day,\n",
        "                                       day+1, day+2, day+3, day+4, day+5, day+6])\n",
        "        test_days_extended = np.array(test_days_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected y_doy and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_doy'].isin(test_days_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 9:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_month')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 250)\n",
        "        test_months = np.random.choice(valid_groups['y_month'].unique(), size=7, replace=True)\n",
        "        # Filter test_data and ensure each group has all samples for the selected y_doy and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_month'].isin(test_months)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 10:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_month')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 250)\n",
        "        test_months = np.random.choice(valid_groups['y_month'].unique(), size=4, replace=True)\n",
        "        # Include ten days before and after the selected DOY\n",
        "        test_months_extended = []\n",
        "        for Month in test_months:\n",
        "            test_months_extended.extend([Month, Month+1])\n",
        "        test_months_extended = np.array(test_months_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected DOY and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_month'].isin(test_months_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 11:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_month')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 250)\n",
        "        test_months = np.random.choice(valid_groups['y_month'].unique(), size=3, replace=True)\n",
        "        # Include ten days before and after the selected DOY\n",
        "        test_months_extended = []\n",
        "        for Month in test_months:\n",
        "            test_months_extended.extend([Month-1, Month, Month+1])\n",
        "        test_months_extended = np.array(test_months_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected DOY and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_month'].isin(test_months_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 12:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_month')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 250)\n",
        "        test_months = np.random.choice(valid_groups['y_month'].unique(), size=2, replace=True)\n",
        "        # Include ten days before and after the selected DOY\n",
        "        test_months_extended = []\n",
        "        for Month in test_months:\n",
        "            test_months_extended.extend([Month-1, Month, Month+1, Month+2])\n",
        "        test_months_extended = np.array(test_months_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected DOY and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_month'].isin(test_months_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 13:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_month')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 250)\n",
        "        test_months = np.random.choice(valid_groups['y_month'].unique(), size=2, replace=True)\n",
        "        # Include ten days before and after the selected DOY\n",
        "        test_months_extended = []\n",
        "        for Month in test_months:\n",
        "            test_months_extended.extend([Month-2, Month-1, Month, Month+1, Month+2])\n",
        "        test_months_extended = np.array(test_months_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected DOY and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_month'].isin(test_months_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 14:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('y_month')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 200)\n",
        "        test_months = np.random.choice(valid_groups['y_month'].unique(), size=2, replace=True)\n",
        "        # Include ten days before and after the selected DOY\n",
        "        test_months_extended = []\n",
        "        for Month in test_months:\n",
        "            test_months_extended.extend([Month-2, Month-1, Month, Month+1, Month+2, Month+3])\n",
        "        test_months_extended = np.array(test_months_extended)\n",
        "        # Filter test_data and ensure each group has all samples for the selected DOY and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['y_month'].isin(test_months_extended)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "\n",
        "    elif test_scenario == 15:\n",
        "        # Generate test_days with groups having at least one sample\n",
        "        df_idx = df.reset_index(drop=True)\n",
        "        grouped_df = df_idx.groupby('Year')\n",
        "        valid_groups = grouped_df.filter(lambda x: len(x) >= 4000)\n",
        "        test_year = np.random.choice(valid_groups['Year'].unique(), size=1, replace=True)\n",
        "        # Filter test_data and ensure each group has all samples for the selected DOY and the adjacent days\n",
        "        test_data = valid_groups[valid_groups['Year'].isin(test_year)].copy()\n",
        "        # Filter train_data based on the test_data indices\n",
        "        train_data = df_idx.drop(test_data.index)\n",
        "        print(\"train_data\", train_data.shape, \"test_data\", test_data.shape)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid test_scenario value. Choose a value between 1 and 15.\")\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "# Define a function to train and evaluate the RF regression model\n",
        "def train_and_evaluate_model(train_data, test_data, Features):\n",
        "    print(Aim)\n",
        "    X_train = train_data[Features]\n",
        "    y_train = train_data[Aim]\n",
        "    X_test = test_data[Features]\n",
        "    y_test = test_data[Aim]\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200, 400],\n",
        "        'max_depth': [3, 5, 8, 10],\n",
        "        'learning_rate': [0.01,0.05,0.1]\n",
        "    }\n",
        "    # Train GBR model with grid search and cross-validation\n",
        "    gbr_model = GradientBoostingRegressor()\n",
        "    grid_search = GridSearchCV(gbr_model, param_grid, cv=4, scoring='r2')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    model = grid_search.best_estimator_\n",
        "    y_pred = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return rmse, r2\n",
        "\n",
        "print('done')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LKpkqh52o8o9",
        "EkkqgGvQFYn9",
        "e1iu6tmAFDLz",
        "ja1YXbbvGHdd",
        "qm1hOyq0GS1w",
        "C-sCftr4GT3c",
        "S5015GTXGVK5",
        "NqlGvVIKFOVP",
        "VwF5Dwo2N_2A",
        "jeSiJTyI-r3G",
        "ehgdlJRdfwfi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}